{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'data_dogscats/train/'\n",
    "TEST_DIR = 'data_dogscats/test/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 96; # 96x96.  Also, 224, 96, 64, and 32 are also common\n",
    "CHANNELS = 3\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "# for small-sample testing\n",
    "TRAINING_AND_VALIDATION_SIZE_DOGS = 500 \n",
    "TRAINING_AND_VALIDATION_SIZE_CATS = 500\n",
    "TRAINING_AND_VALIDATION_SIZE_ALL  = 1000\n",
    "TRAINING_SIZE = 700  # TRAINING_SIZE + VALID_SIZE must equal TRAINING_AND_VALIDATION_SIZE_ALL\n",
    "VALID_SIZE = 300\n",
    "TEST_SIZE_ALL = 200\n",
    "\n",
    "\n",
    "\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] \n",
    "train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "test_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "train_images = train_dogs[:TRAINING_AND_VALIDATION_SIZE_DOGS] + train_cats[:TRAINING_AND_VALIDATION_SIZE_CATS]\n",
    "train_labels = np.array ((['dogs'] * TRAINING_AND_VALIDATION_SIZE_DOGS) + (['cats'] * TRAINING_AND_VALIDATION_SIZE_CATS))\n",
    "test_images =  test_images[:TEST_SIZE_ALL]\n",
    "test_labels = np.array (['unknownclass'] * TEST_SIZE_ALL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 1000\n",
      "Processed 250 of 1000\n",
      "Processed 500 of 1000\n",
      "Processed 750 of 1000\n",
      "Processed 0 of 200\n",
      "Train shape: (1000, 96, 96, 3)\n",
      "Test shape: (200, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "def read_image(file_path):\n",
    "    img = Image.open(file_path) #cv2.IMREAD_GRAYSCALE\n",
    "    img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, IMAGE_SIZE, IMAGE_SIZE, CHANNELS), dtype=np.float32)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file);\n",
    "        image_data = np.array (image, dtype=np.float32);\n",
    "        image_data[:,:,0] = (image_data[:,:,0].astype(float) - pixel_depth / 2) / pixel_depth\n",
    "        image_data[:,:,1] = (image_data[:,:,1].astype(float) - pixel_depth / 2) / pixel_depth\n",
    "        image_data[:,:,2] = (image_data[:,:,2].astype(float) - pixel_depth / 2) / pixel_depth\n",
    "        \n",
    "        data[i] = image_data; # image_data.T\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))    \n",
    "    return data\n",
    "\n",
    "train_normalized = prep_data(train_images)\n",
    "test_normalized = prep_data(test_images)\n",
    "\n",
    "print(\"Train shape: {}\".format(train_normalized.shape))\n",
    "print(\"Test shape: {}\".format(test_normalized.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115af5908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115af5a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115af5908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow (train_normalized[0,:,:,:], interpolation='nearest')\n",
    "plt.figure ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (700, 96, 96, 3) (700,)\n",
      "Validation (300, 96, 96, 3) (300,)\n",
      "Test (200, 96, 96, 3) (200,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed (133)\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset_rand, train_labels_rand = randomize(train_normalized, train_labels)\n",
    "test_dataset, test_labels = randomize(test_normalized, test_labels)\n",
    "\n",
    "# split up into training + valid\n",
    "valid_dataset = train_dataset_rand[:VALID_SIZE,:,:,:]\n",
    "valid_labels =   train_labels_rand[:VALID_SIZE]\n",
    "train_dataset = train_dataset_rand[VALID_SIZE:VALID_SIZE+TRAINING_SIZE,:,:,:]\n",
    "train_labels  = train_labels_rand[VALID_SIZE:VALID_SIZE+TRAINING_SIZE]\n",
    "print ('Training', train_dataset.shape, train_labels.shape)\n",
    "print ('Validation', valid_dataset.shape, valid_labels.shape)\n",
    "print ('Test', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (700, 96, 96, 3) (700, 2)\n",
      "Validation set (300, 96, 96, 3) (300, 2)\n",
      "Test set (200, 96, 96, 3) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "image_size = IMAGE_SIZE \n",
    "num_labels = 2\n",
    "num_channels = 3 # rg\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (labels=='cats').astype(np.float32); # set dogs to 0 and cats to 1\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print ('Training set', train_dataset.shape, train_labels.shape)\n",
    "print ('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print ('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "  tf.float32, shape=[None, image_size*image_size])\n",
    "  tf_train_dataset = tf.reshape(tf_train_dataset, [-1, image_size, image_size, num_channels])\n",
    "\n",
    "\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=[None, num_labels])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [int (image_size / 4 * image_size / 4 * depth), num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(data, labels, batch_size, batch_id):\n",
    "    batch_data = data[batch_id:min(batch_id+ batch_size, len(data))]\n",
    "    batch_labels = labels[batch_id:min(batch_id + batch_size, len(data))] \n",
    "    new_batch_id = (batch_id + batch_size) % (len(data) - batch_size)\n",
    "    \n",
    "    return batch_data, batch_labels, new_batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-54-477ea39fe69a>:6 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 50 : 0.676375\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 100 : 0.6694\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 150 : 1.01638\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 52.3%\n",
      "Minibatch loss at step 200 : 0.534816\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 52.0%\n",
      "Minibatch loss at step 250 : 0.468226\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 300 : 0.655348\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 56.0%\n",
      "Minibatch loss at step 350 : 0.534322\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 59.3%\n",
      "Minibatch loss at step 400 : 0.334389\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 450 : 0.403122\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 500 : 0.151555\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 55.0%\n",
      "Minibatch loss at step 550 : 0.101122\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 600 : 0.0787875\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 55.3%\n",
      "Minibatch loss at step 650 : 0.293962\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 53.7%\n",
      "Minibatch loss at step 700 : 0.036985\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.0%\n",
      "Minibatch loss at step 750 : 0.0157999\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 58.7%\n",
      "Minibatch loss at step 800 : 0.0043153\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-477ea39fe69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch loss at step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch accuracy: %.1f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation accuracy: %.1f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \"\"\"\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3631\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3633\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "   return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "num_steps = 1001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    batch_id = 0\n",
    "    step = 1\n",
    "    while step  < 1001:\n",
    "        batch_data, batch_labels, new_batch_id = next_batch(train_dataset,train_labels, batch_size, batch_id)\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        batch_id = new_batch_id \n",
    "       \n",
    "        if (step % 50 == 0):\n",
    "            print (\"Minibatch loss at step\", step, \":\", l)\n",
    "            print (\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print (\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        step +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-921f3f5fcb3b>:6 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0 : 2.14413\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 52.7%\n",
      "Minibatch loss at step 50 : 0.680309\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 48.7%\n",
      "Minibatch loss at step 100 : 0.688593\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 44.3%\n",
      "Minibatch loss at step 150 : 0.773962\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 49.0%\n",
      "Minibatch loss at step 200 : 0.743205\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 55.0%\n",
      "Minibatch loss at step 250 : 0.480577\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 56.3%\n",
      "Minibatch loss at step 300 : 0.606706\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 350 : 0.354787\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 56.3%\n",
      "Minibatch loss at step 400 : 0.449632\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 55.3%\n",
      "Minibatch loss at step 450 : 0.490285\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 52.7%\n",
      "Minibatch loss at step 500 : 0.749936\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 55.0%\n",
      "Minibatch loss at step 550 : 0.430205\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 53.7%\n",
      "Minibatch loss at step 600 : 0.185427\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 56.0%\n",
      "Minibatch loss at step 650 : 0.278102\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 58.7%\n",
      "Minibatch loss at step 700 : 0.113201\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 58.0%\n",
      "Minibatch loss at step 750 : 0.0867041\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 54.7%\n",
      "Minibatch loss at step 800 : 0.225381\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 850 : 0.132944\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 59.7%\n",
      "Minibatch loss at step 900 : 0.0762467\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 54.7%\n",
      "Minibatch loss at step 950 : 0.0385233\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 59.3%\n",
      "Minibatch loss at step 1000 : 0.0392571\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 56.3%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "   return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "num_steps = 1001\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print (\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print (\"Minibatch loss at step\", step, \":\", l)\n",
    "      print (\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print (\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "tf_train_dataset = tf.placeholder(\n",
    "tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = image_size * image_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (image_size, image_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 3\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_train_dataset = tf.placeholder(\n",
    "tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "tf_valid_dataset = tf.constant(valid_dataset)\n",
    "tf_test_dataset = tf.constant(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=2,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(tf_train_labels, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_test_dataset.va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch_size = 16\n",
    "\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(test_dataset)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "        offset = (i * batch_size) % (test_labels.shape[0] - batch_size)\n",
    "        batch_data = test_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = test_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        \"\"\"images = test_dataset[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = test_labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {tf_train_dataset: images,\n",
    "                     tf_train_labels: labels}\"\"\"\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = test_labels.values\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # Plot the confusion matrix, if desired.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(16, 2)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_3' with dtype float and shape [16,2]\n\t [[Node: Placeholder_3 = Placeholder[dtype=DT_FLOAT, shape=[16,2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_3', defined at:\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-3db2c5fd77b2>\", line 4, in <module>\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_3' with dtype float and shape [16,2]\n\t [[Node: Placeholder_3 = Placeholder[dtype=DT_FLOAT, shape=[16,2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_3' with dtype float and shape [16,2]\n\t [[Node: Placeholder_3 = Placeholder[dtype=DT_FLOAT, shape=[16,2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-e5d6518e120b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print_test_accuracy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-73a97a1d7b8d>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(num_iterations)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# TensorFlow assigns the variables in feed_dict_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# to the placeholder variables and then runs the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Print status every 100 iterations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_3' with dtype float and shape [16,2]\n\t [[Node: Placeholder_3 = Placeholder[dtype=DT_FLOAT, shape=[16,2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_3', defined at:\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-3db2c5fd77b2>\", line 4, in <module>\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_3' with dtype float and shape [16,2]\n\t [[Node: Placeholder_3 = Placeholder[dtype=DT_FLOAT, shape=[16,2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "optimize(100)\n",
    "#print_test_accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
